year,title,authors,publication,link,category,demo,data,software,bibtex,abstract,poster,posterTitle,posterAuthors,posterConference,posterYear,posterLocation,pageCategory,download,downloadName,downloadDescription,downloadLink,downloadDate,downloadLinkNames,new,notes
2023,Navigating Data Scarcity: Pretraining for Medical Utterance Classification,"DoJune Min, Verónica Pérez-Rosas, Rada Mihalcea",Proceedings of the 5th Clinical Natural Language Processing Workshop 2023,https://aclanthology.org/2023.clinicalnlp-1.8.pdf,"NLP for Healthcare, Conversational Technologies",,,,"@inproceedings{min-etal-2023-navigating,     title = ""Navigating Data Scarcity: Pretraining for Medical Utterance Classification"",     author = ""Min, Do June  and       Perez-Rosas, Veronica  and       Mihalcea, Rada"",     booktitle = ""Proceedings of the 5th Clinical Natural Language Processing Workshop"",     month = jul,     year = ""2023"",     address = ""Toronto, Canada"",     publisher = ""Association for Computational Linguistics"",     url = ""https://aclanthology.org/2023.clinicalnlp-1.8"",     doi = ""10.18653/v1/2023.clinicalnlp-1.8"",     pages = ""59--68"", }","  Pretrained language models leverage self-supervised learning to use large amounts of unlabeled text for learning contextual representations of sequences. However, in the domain of medical conversations, the availability of large, public datasets is limited due to issues of privacy and data management. In this paper, we study the effectiveness of dialog-aware pretraining objectives and multiphase training in using unlabeled data to improve LMs training for medical utterance classification. The objectives of pretraining for dialog awareness involve tasks that take into account the structure of conversations, including features such as turn-taking and the roles of speakers. The multiphase training process uses unannotated data in a sequence that prioritizes similarities and connections between different domains. We empirically evaluate these methods on conversational dialog classification tasks in the medical and counseling domains, and find that multiphase training can help achieve higher performance than standard pretraining or finetuning.",,,,,,,SunLab,FALSE,,,,,,TRUE,
2023,Emergent Linear Representations in World Models of Self-Supervised Sequence Models,"Neel Nanda, Andrew Lee, Martin Wattenberg",,https://arxiv.org/pdf/2309.00941.pdf,"Conversational Technologies, Language Modelling",,,,"@article{nanda2023emergent,   title={Emergent Linear Representations in World Models of Self-Supervised Sequence Models},   author={Nanda, Neel and Lee, Andrew and Wattenberg, Martin},   journal={arXiv preprint arXiv:2309.00941},   year={2023} }","How do sequence models represent their decision-making process? Prior work suggests that Othello-playing neural network learned nonlinear models of the board state (Li et al., 2023). In this work, we provide evidence of a closely related linear representation of the board. In particular, we show that probing for ""my colour"" vs. ""opponent's colour"" may be a simple yet powerful way to interpret the model's internal state. This precise understanding of the internal representations allows us to control the model's behaviour with simple vector arithmetic. Linear representations enable significant interpretability progress, which we demonstrate with further exploration of how the world model is computed.",,,,,,,SunLab,FALSE,,,,,,TRUE,
2023,Scalable Performance Analysis for Vision-Language Models,"Santiago Castro*, Oana Ignat*, Rada Mihalcea (* = equal contribution)",*SEM 2023,https://aclanthology.org/2023.starsem-1.26.pdf,Vision and Text,,,,"@inproceedings{castro-etal-2023-scalable,</br>
    title = ""Scalable Performance Analysis for Vision-Language Models"",</br>
    author = ""Castro, Santiago  and</br>
      Ignat, Oana  and</br>
      Mihalcea, Rada"",</br>
    booktitle = ""Proceedings of the The 12th Joint Conference on Lexical and Computational Semantics (*SEM 2023)"",</br>
    month = jul,</br>
    year = ""2023"",</br>
    address = ""Toronto, Canada"",</br>
    publisher = ""Association for Computational Linguistics"",</br>
    url = ""https://aclanthology.org/2023.starsem-1.26"",</br>
    pages = ""284--294"",</br>
    abstract = ""Joint vision-language models have shown great performance over a diverse set of tasks. However, little is known about their limitations, as the high dimensional space learned by these models makes it difficult to identify semantic errors. Recent work has addressed this problem by designing highly controlled probing task benchmarks. Our paper introduces a more scalable solution that relies on already annotated benchmarks. Our method consists of extracting a large set of diverse features from a vision-language benchmark and measuring their correlation with the output of the target model. We confirm previous findings that CLIP behaves like a bag of words model and performs better with nouns and verbs; we also uncover novel insights such as CLIP getting confused by concrete words. Our framework is available at https://github.com/MichiganNLP/Scalable-VLM-Probing and can be used with other multimodal models and benchmarks."",</br>
}","Joint vision-language models have shown great performance over a diverse set of tasks. However, little is known about their limitations, as the high dimensional space learned by these models makes it difficult to identify semantic errors. Recent work has addressed this problem by designing highly controlled probing task benchmarks. Our paper introduces a more scalable solution that relies on already annotated benchmarks. Our method consists of extracting a large set of diverse features from a vision-language benchmark and measuring their correlation with the output of the target model. We confirm previous findings that CLIP behaves like a bag of words model and performs better with nouns and verbs; we also uncover novel insights such as CLIP getting confused by concrete words. Our framework is available at https://github.com/MichiganNLP/Scalable-VLM-Probing and can be used with other multimodal models and benchmarks.",probing_clip_poster_2023,Scalable Performance Analysis for Vision-Language Models,"Santiago Castro*, Oana Ignat*, Rada Mihalcea (* = equal contribution)",*SEM,2023,"Toronto, Canada",SunLab,TRUE,Scalable Vision-Language Model Probing (code),Framework to probe Vision-Language Models on language properties,https://github.com/MichiganNLP/Scalable-VLM-Probing,19-Jun-23,Code,TRUE,
